{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7c1dc2",
   "metadata": {},
   "source": [
    "# Temperature Profile Classification - 2 Class system - Full DataSet Load\n",
    "GMM classification of Southern Ocean Argo float temperature profile data. This notebook uses a previously created model, PCA and sample data.<br><br>\n",
    "### Dask import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2963a54",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dask server setup cell\n",
    "'''\n",
    "target_version='0.19.0'\n",
    "!pip install xarray=={target_version} --upgrade #--upgrade\n",
    "\n",
    "import logging\n",
    "import subprocess\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "from distributed import WorkerPlugin\n",
    "\n",
    "import dask\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    " \n",
    "class PipPlugin(WorkerPlugin):\n",
    "    \"\"\"\n",
    "    Install packages on a worker as it starts up.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    packages : List[str]\n",
    "        A list of packages to install with pip on startup.\n",
    "    \"\"\"\n",
    "    def __init__(self, packages):\n",
    "        self.packages = packages\n",
    " \n",
    "    def setup(self, worker):\n",
    "        logger = logging.getLogger(\"distributed.worker\")\n",
    "        subprocess.call(['python', '-m', 'pip', 'install', '--upgrade'] + self.packages)\n",
    "        logger.info(\"Installed %s\", self.packages)\n",
    "        \n",
    "def check():\n",
    "    import xarray\n",
    "    return xarray.__version__\n",
    "        \n",
    "gateway = Gateway()\n",
    "cluster = gateway.new_cluster(worker_memory=8)\n",
    "cluster.scale(20)\n",
    "client = Client(cluster)\n",
    "client\n",
    " \n",
    "plugin = PipPlugin([f'xarray=={target_version}'])\n",
    "client.register_worker_plugin(plugin)\n",
    "client.run(check)\n",
    "'''\n",
    "blank=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610deb0d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202468ad",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230b0c3",
   "metadata": {},
   "source": [
    "### Choices for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a9e6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment data for analysis\n",
    "dataVariableId = 'thetao'\n",
    "dataExperimentId = 'historical'\n",
    "dataSourceId = 'UKESM1-0-LL'\n",
    "dataInstitutionId = 'MOHC'\n",
    "approvedIds = [\"r2i1p1f2\"] #insert start of approved member_ids\n",
    "\n",
    "#File imports\n",
    "maskName = \"OceanMaskVolcello\"\n",
    "modelName = \"GMM_UK_2Class_R1\"\n",
    "\n",
    "#Data definitions\n",
    "startDate = '1980-01'\n",
    "endDate = '2009-12'\n",
    "timeRange = slice(startDate, endDate)\n",
    "levSel = slice(0, 2000) #Selected levels to be investigated\n",
    "maxLat = -30 #Selected latitude to be investigated\n",
    "runIdSel = 0\n",
    "maskEnable = False #Decides if training data mask is applied, or if full data set is classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1bbf7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f61e6",
   "metadata": {},
   "source": [
    "### Libaries and Modules\n",
    "Importing the necessary libaries and modules for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf4d1068",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "#Import cell\n",
    "import calendar\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cfeature\n",
    "import dask.dataframe as dd\n",
    "import fsspec\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl ###\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import matplotlib.ticker as ticker\n",
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "from dask import config\n",
    "from dask import delayed\n",
    "from joblib import dump, load\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn import mixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "config.set(**{'array.slicing.split_large_chunks': True})\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a452bba",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224d2e6",
   "metadata": {},
   "source": [
    "### Importing data sets\n",
    "Importing the data for the models.\n",
    "\n",
    "<b>Import sample data set and corresponding time/geo data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820d80ab",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 UKESM1-0-LL data sets opened\n",
      "Data sets successfully merged and renamed into dataRaw. Data dimensions are Frozen({'RunId': 1, 'time': 1980, 'lev': 75, 'j': 330, 'i': 360}).\n"
     ]
    }
   ],
   "source": [
    "#Importing UK ESM data cell\n",
    "\n",
    "#Selecting data tables\n",
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "dfFilt = df[df.variable_id.eq(dataVariableId) & df.experiment_id.eq(dataExperimentId) & df.source_id.eq(dataSourceId) & df.institution_id.eq(dataInstitutionId)]\n",
    "\n",
    "memberArr = np.empty(shape=(0), dtype=bool)\n",
    "for i in dfFilt[\"member_id\"]:\n",
    "    rowSel = i[:] in approvedIds #adapt i[:] to match size of approvedIds\n",
    "    memberArr = np.append(memberArr, rowSel)\n",
    "\n",
    "memberSer = pd.Series(memberArr, name='bools')\n",
    "dfFilt = dfFilt[memberSer.values]\n",
    "\n",
    "#Opening and counting number of tables\n",
    "fileSetList = []\n",
    "for i in range(len(dfFilt)):\n",
    "    zstore = dfFilt.zstore.values[i]\n",
    "    mapper = fsspec.get_mapper(zstore)\n",
    "    fileRaw = xr.open_zarr(mapper, consolidated=True)\n",
    "    fileSetList.append(fileRaw)\n",
    "fileCount = len(fileSetList)\n",
    "if fileCount:\n",
    "    print(str(fileCount)+\" \"+dataSourceId+\" data sets opened\")\n",
    "else:\n",
    "    print(\"No UKESM data sets opened\")\n",
    "    \n",
    "#Formatting dates into np.datetime64 format\n",
    "for i in range(fileCount): \n",
    "    startDateIterate = np.datetime64(fileSetList[i]['time'].values[0],'M')\n",
    "    endDateIterate = np.datetime64(fileSetList[i]['time'].values[-1],'M') + np.timedelta64(1,'M')\n",
    "    fileSetList[i]['time']=('time', np.arange(startDateIterate, endDateIterate, dtype='datetime64[M]'))\n",
    "    fileSetList[i]['time_bnds']=('time_bnds', np.arange(startDateIterate, endDateIterate, dtype='datetime64[M]')) \n",
    "fileSet = xr.combine_nested(fileSetList, concat_dim='RunId') #Combining data sets\n",
    "\n",
    "dataRaw = fileSet.thetao\n",
    "try: #Adjusting array names\n",
    "    dataRaw = dataRaw.rename({\"latitude\":\"lat\", \"longitude\":\"lon\"})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Data sets successfully merged and renamed into dataRaw. Data dimensions are \"+str(dataRaw.sizes)+\".\")\n",
    "#dataRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771e1b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_id</th>\n",
       "      <th>institution_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>table_id</th>\n",
       "      <th>variable_id</th>\n",
       "      <th>grid_label</th>\n",
       "      <th>zstore</th>\n",
       "      <th>dcpp_init_year</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214481</th>\n",
       "      <td>CMIP</td>\n",
       "      <td>MOHC</td>\n",
       "      <td>UKESM1-0-LL</td>\n",
       "      <td>historical</td>\n",
       "      <td>r2i1p1f2</td>\n",
       "      <td>Omon</td>\n",
       "      <td>thetao</td>\n",
       "      <td>gn</td>\n",
       "      <td>gs://cmip6/CMIP6/CMIP/MOHC/UKESM1-0-LL/histori...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20190708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity_id institution_id    source_id experiment_id member_id  \\\n",
       "214481        CMIP           MOHC  UKESM1-0-LL    historical  r2i1p1f2   \n",
       "\n",
       "       table_id variable_id grid_label  \\\n",
       "214481     Omon      thetao         gn   \n",
       "\n",
       "                                                   zstore  dcpp_init_year  \\\n",
       "214481  gs://cmip6/CMIP6/CMIP/MOHC/UKESM1-0-LL/histori...             NaN   \n",
       "\n",
       "         version  \n",
       "214481  20190708  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d704971c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#UK ESM raw processing cell\n",
    "dfESMLev = dataRaw.sel(lev=levSel) #Selects level data down to 2k\n",
    "dfESMLevT = dfESMLev.sel(time=timeRange)\n",
    "dfESMLatLevT = dfESMLevT.where(dfESMLevT.lat < maxLat, drop=True) #Selection of latitude\n",
    "dfESMLatLevT = dfESMLatLevT.squeeze()\n",
    "#dfESMLatLevT = dfESMLatLevT.reset_coords(drop=True) #Removes lev if single value\n",
    "\n",
    "globalStartDate = dfESMLatLevT[\"time\"][0].values\n",
    "globalDateInc = dfESMLatLevT[\"time\"][1].values - globalStartDate\n",
    "#np.datetime64(globalDateInc,'D')\n",
    "globalEndDateIn = dfESMLatLevT[\"time\"][-1].values\n",
    "globalEndDateOut = globalEndDateIn + globalDateInc\n",
    "\n",
    "globalStartDateStr = str(globalStartDate)[:7]\n",
    "globalEndDateInStr = str(globalEndDateIn)[:7]\n",
    "globalEndDateOutStr = str(globalEndDateOut)[:7]\n",
    "\n",
    "print(\"UKESM data loaded and stored in dfESMLatLevT. Data dimensions are \"+str(dfESMLatLevT.sizes)+\".\")\n",
    "#dfESMLatLevT #Uncomment to see data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf63dca",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Loading ocean Masks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd688a7",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Ocean mask import cell\n",
    "maskFile = xr.open_dataset(maskName)\n",
    "oceanMask = maskFile.to_array()\n",
    "maskFile = xr.open_dataset(\"OceanMaskUKESM1\")\n",
    "oceanMask2 = maskFile.to_array()\n",
    "print(\"Mask Loaded and stored in oceanMask and oceanMask2 (volcello and UKESM).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff8060",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Unpacking ocean masks</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0f0d9",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Mask unpacking cell\n",
    "geoRange = oceanMask #copying mask\n",
    "geoRange = geoRange.rename({\"variable\":\"cleanMe\"}) #Dimension removal\n",
    "geoRange = geoRange.sel(cleanMe = geoRange.cleanMe.values[0]) #Dimension removal\n",
    "geoRange = geoRange.reset_coords(\"cleanMe\", drop=True) #Dimension removal\n",
    "geoRangeS = geoRange.stack(ij =(\"i\", \"j\")) #Stacking\n",
    "geoRangeFilt = geoRangeS.dropna(\"ij\")\n",
    "print(\"Ocean mask unpacked into geoRangeFilt.\")\n",
    "\n",
    "geoRange2 = oceanMask2 #copying mask\n",
    "geoRange2S = geoRange2.stack(ij =(\"i\", \"j\")) #Stacking\n",
    "geoRangeFilt2 = geoRange2S.dropna(\"ij\")\n",
    "print(\"UKESM Ocean mask unpacked into geoRangeFilt2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd3c7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Date Calculations</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59086c",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Date calculation cell\n",
    "startDateNp = np.datetime64(startDate, 'M')\n",
    "endDateNp = np.datetime64(endDate, 'M')\n",
    "timeDiff = endDateNp - startDateNp\n",
    "timeDiff = timeDiff.astype(int) + 1\n",
    "print(\"Calculated date range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa8740",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5dbebb",
   "metadata": {},
   "source": [
    "### Calculation functions\n",
    "<b>Functions:</b><br>\n",
    "<ul>\n",
    "<li>pickRand - Takes in data frame and returns sampled data frame with a randomly selected number of rows from the input data frame, controled by the second input variable to the function.\n",
    "<li>storeMeta - Returns a np array containing the latitude and longitude data for an input xarray and associated ij.\n",
    "<li>loadModel - loadeds and returns GMM model named in input.\n",
    "<li>saveModel - saves input GMM model to provided name, if no name provided default is GMMGenerated.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba921db",
   "metadata": {
    "code_folding": [
     0,
     1,
     18,
     37,
     48,
     60
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Calculation functions cell\n",
    "def pickRand(dataArray, sampleFactor):\n",
    "    '''Returns a sample of the input array, size of sampled array is based on sampleFactor. For factor > 1 that many points are chosen, for factor < 1 that % is taken of the array'''\n",
    "    arrLen = len(dataArray)\n",
    "    if sampleFactor > 1:\n",
    "        sampleSize = int(sampleFactor)\n",
    "    elif sampleFactor > 0:\n",
    "        sampleSize = int(sampleFactor*arrLen)\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    filtArr = np.zeros(arrLen, dtype=bool) # empty mask\n",
    "    sampleId = np.random.choice(arrLen, sampleSize, False) # np array of randomly generated non repeating numbers\n",
    "    for i in sampleId:\n",
    "        filtArr[i] = True # populating mask\n",
    "    return dataArray[filtArr] # applies mask\n",
    "\n",
    "\n",
    "def pickRandMask(maskLen, maskQuantity, sampleFactor):\n",
    "    '''Returns a linear mask for the input dimensions, size of mask is based on sampleFactor. For factor > 1 that many points are chosen, for factor < 1 that % is taken of the array'''\n",
    "    if sampleFactor > 1:\n",
    "        sampleSize = int(sampleFactor)\n",
    "    elif sampleFactor > 0:\n",
    "        sampleSize = int(sampleFactor*maskLen)\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    globalArr = np.empty(shape=(0), dtype=bool)\n",
    "    for i in range(maskQuantity):\n",
    "        filtArr = np.zeros(maskLen, dtype=bool) # empty mask\n",
    "        sampleId = np.random.choice(maskLen, sampleSize, False) # np array of randomly generated non repeating numbers\n",
    "        for j in sampleId:\n",
    "            filtArr[j] = True # populating mask\n",
    "        globalArr = np.append(globalArr, filtArr)\n",
    "    return globalArr\n",
    "\n",
    "\n",
    "def storeMeta(dataArray):\n",
    "    '''Returns a np array containing the latitude and longitude data for the input xarray and the associated ij index'''\n",
    "    storeLen = len(dataArray[\"lat\"]) # assumes each lat has a corresponding lon\n",
    "    storage = np.empty(shape=(0,storeLen))\n",
    "    storage = np.append(storage, [dataArray[\"lat\"].values], axis = 0)\n",
    "    storage = np.append(storage, [dataArray[\"lon\"].values], axis = 0)\n",
    "    #storage = np.append(storage, [dataArray[\"time\"].values], axis = 0)\n",
    "    #storage = np.append(storage, [dataArray[\"ij\"].values], axis = 0)\n",
    "    return storage\n",
    "\n",
    "\n",
    "def loadModel(modelName:str):\n",
    "    '''Loades the input GMM model named in the functions input. Returns loaded model.'''\n",
    "    means = np.load(modelName + '_means.npy')\n",
    "    covar = np.load(modelName + '_covariances.npy')\n",
    "    GMModel = mixture.GaussianMixture(n_components = len(means), covariance_type='full')\n",
    "    GMModel.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(covar))\n",
    "    GMModel.weights_ = np.load(modelName + '_weights.npy')\n",
    "    GMModel.means_ = means\n",
    "    GMModel.covariances_ = covar\n",
    "    return GMModel\n",
    "\n",
    "\n",
    "def saveModel(GMModel, modelName = \"GMMGenerated\"):\n",
    "    '''Saves the input GMM model's weights, means and covariances. Assigns input name if provided to model.'''\n",
    "    GMModel_name = str(modelName)\n",
    "    np.save(modelName + '_weights', GMModel.weights_, allow_pickle=False)\n",
    "    np.save(modelName + '_means', GMModel.means_, allow_pickle=False)\n",
    "    np.save(modelName + '_covariances', GMModel.covariances_, allow_pickle=False)\n",
    "    return 0\n",
    "\n",
    "print(\"Calculation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe44f97",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb12311",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "<b>Functions:</b>\n",
    "<ul>\n",
    "<li> bicPlot - Plots BIC score array against component number.\n",
    "<li> locationPlotGroup - plots location and classification of data points for an input numpy array.\n",
    "<li> locationPlotGroupDF - plots location and classification of data points for an input data frame.\n",
    "<li> locationPlotGroupDFMonthly - plots location and classification of data points for an input data frame in monthly subplots.\n",
    "<li> locationPlotTime - plots locations of an input data array on a map with a colour scale for time.\n",
    "<li> locationPlotUncertaintyDF - plots uncertainty in classification on a location plot.\n",
    "<li> tempPointPlot - Plots the temperature profile of a single point against depth.\n",
    "<li> tempGroupPlot - Plots the mean/+-1std temperature profiles of all classes in input dataArrays (seperate mean and std).\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66336863",
   "metadata": {
    "code_folding": [
     0,
     3,
     25,
     39,
     56,
     70,
     89,
     106,
     120,
     140,
     153,
     166,
     175
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Plotting functions Cell\n",
    "sampleDepthAxis = dfESMLatLevT[\"lev\"]\n",
    "\n",
    "def bicPlot(bicArray, startNo, endNo, skipNo, title, label, plotNo):\n",
    "    '''Plots input BIC score array'''\n",
    "    plt.figure(plotNo, figsize=(20, 8))\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "    componentRange = range(startNo, endNo, skipNo)\n",
    "    plt.plot(componentRange, bicArray, label = str(label))\n",
    "    \n",
    "    bicArrayMax = np.max(bicArray)\n",
    "    bicArrayMin = np.min(bicArray)\n",
    "    bicRange = bicArrayMax-bicArrayMin\n",
    "    if bicRange == 0:\n",
    "        bicRange = 20 #provides border 1 if all bic values are identical\n",
    "    plt.xticks(componentRange)\n",
    "    plt.xlim([startNo-0.5, endNo+0.5])\n",
    "    plt.ylim([bicArrayMin-0.05*bicRange, bicArrayMax+0.05*bicRange])\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"BIC score\")\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "def locationPlotGroup(metaDataArray, size, plotNo):\n",
    "    '''Plots locations of numpy arrays with group colour scheme'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(metaDataArray[1], metaDataArray[0], transform=ccrs.PlateCarree(), c =  metaDataArray[3], cmap='RdBu_r')\n",
    "    cb = plt.colorbar(im)\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(\"Grouped Sample Locations (\"+str(len(metaDataArray[0]))+\")\")\n",
    "\n",
    "\n",
    "def locationPlotGroupDFTime(dataFrame, title, size, plotNo):\n",
    "    '''Plots locations of data frame points with group colour scheme'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataFrame[\"lon\"], dataFrame[\"lat\"], transform=ccrs.PlateCarree(), c =  mdates.date2num(dataFrame[\"time\"]), cmap='brg')\n",
    "    cb = plt.colorbar(im)\n",
    "    loc = mdates.AutoDateLocator()\n",
    "    cb.ax.yaxis.set_major_locator(loc)\n",
    "    cb.ax.yaxis.set_major_formatter(mdates.ConciseDateFormatter(loc))\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(str(title))\n",
    "    \n",
    "    \n",
    "def locationPlotGroupDFLab(dataFrame, title, size, plotNo):\n",
    "    '''Plots locations of data frame points with group colour scheme'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataFrame[\"lon\"], dataFrame[\"lat\"], transform=ccrs.PlateCarree(), c =  dataFrame[\"labelSorted\"], cmap='RdBu_r')\n",
    "    cb = plt.colorbar(im)\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(str(title))\n",
    "\n",
    "\n",
    "def locationPlotGroupDFMonthly(dataFrame, title, plotNo):\n",
    "    '''Plots locations of dataframe points by monthly subplot with group colour scheme'''\n",
    "    fig = plt.figure(plotNo, figsize=(30,42))\n",
    "    plt.title(str(title))\n",
    "    for i in range(1, 13):\n",
    "        timeData = dataFrame.where(dataFrame[\"time\"].dt.month==i)\n",
    "        ax = plt.subplot(4, 3, i, projection=ccrs.SouthPolarStereo())\n",
    "        ax.add_feature(cfeature.OCEAN)\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.coastlines()\n",
    "        ax.gridlines()\n",
    "        im = ax.scatter(timeData[\"lon\"], timeData[\"lat\"], transform=ccrs.PlateCarree(), c =  timeData[\"labelSorted\"], cmap='RdBu_r')\n",
    "        plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "        plt.title(calendar.month_abbr[i]) \n",
    "    plt.subplots_adjust(wspace=0, hspace=0.05)\n",
    "    cb_ax = fig.add_axes([0.27, 0.1, 0.5, 0.02])\n",
    "    cbar = fig.colorbar(im, cax=cb_ax, orientation=\"horizontal\")\n",
    "\n",
    "\n",
    "def locationPlotTime(dataArray, size, plotNo):\n",
    "    '''Plots locations of numpy arrays with date colour scheme'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataArray[1], dataArray[0], transform=ccrs.PlateCarree(), c= mdates.date2num(dataArray[2]), cmap='brg')\n",
    "    cb = plt.colorbar(im)\n",
    "    loc = mdates.AutoDateLocator()\n",
    "    cb.ax.yaxis.set_major_locator(loc)\n",
    "    cb.ax.yaxis.set_major_formatter(mdates.ConciseDateFormatter(loc))\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(\"Sample Locations (\"+str(len(dataArray[0]))+\")\")\n",
    "\n",
    "\n",
    "def locationPlotUncertaintyDF(dataFrame, title, size, plotNo):\n",
    "    '''Plots input data array classification uncertainties'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataFrame[\"lon\"], dataFrame[\"lat\"], transform=ccrs.PlateCarree(), c =  dataFrame[\"classUncertainty\"], cmap='Blues')\n",
    "    cb = plt.colorbar(im)\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(str(title))\n",
    "\n",
    "\n",
    "def locationPlotUncertaintyDFMonthly(dataFrame, title, plotNo):\n",
    "    '''Plots locations of dataframe points by monthly subplot with group colour scheme'''\n",
    "    fig = plt.figure(plotNo, figsize=(30,42))\n",
    "    plt.title(str(title))\n",
    "    for i in range(1, 13):\n",
    "        timeData = dataFrame.where(dataFrame[\"time\"].dt.month==i)\n",
    "        ax = plt.subplot(4, 3, i, projection=ccrs.SouthPolarStereo())\n",
    "        ax.add_feature(cfeature.OCEAN)\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.coastlines()\n",
    "        ax.gridlines()\n",
    "        im = ax.scatter(timeData[\"lon\"], timeData[\"lat\"], transform=ccrs.PlateCarree(), c =  timeData[\"classUncertainty\"], cmap='Blues', vmin=0, vmax=1)\n",
    "        #cb = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "        plt.title(calendar.month_abbr[i]) \n",
    "    plt.subplots_adjust(wspace=0, hspace=0.05)\n",
    "    cb_ax = fig.add_axes([0.27, 0.1, 0.5, 0.02])\n",
    "    cbar = fig.colorbar(im, cax=cb_ax, orientation=\"horizontal\")\n",
    "\n",
    "\n",
    "def locationPlotXr(dataArray, size, plotNo):\n",
    "    '''Plots locations of numpy arrays with date colour scheme'''\n",
    "    plt.figure(plotNo, figsize=size)\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataArray[\"lon\"], dataArray[\"lat\"], transform=ccrs.PlateCarree())\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(\"Sample Locations (\"+str(len(dataArray[\"lat\"]))+\")\")        \n",
    "    \n",
    "    \n",
    "def surfaceTempPlot(dataArray, plotNo):\n",
    "    plt.figure(plotNo, figsize=(20,20))\n",
    "    ax = plt.axes(projection=ccrs.SouthPolarStereo())\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.coastlines()\n",
    "    ax.gridlines()\n",
    "    im = ax.scatter(dataArray[\"lon\"], dataArray[\"lat\"], transform=ccrs.PlateCarree(), c =  dataArray[surfaceLevName], cmap='RdBu_r')\n",
    "    cb = plt.colorbar(im)\n",
    "    plt.plot(np.arange(0,361,1),np.ones(361)*-29.5, transform=ccrs.PlateCarree(), color=\"Black\")\n",
    "    plt.title(\"Surface Temperature of Samples\")\n",
    "\n",
    "\n",
    "def tempPointPlot(dataArray, label, title, plotNo):\n",
    "    '''Displays temperature profile plot for a given data set, singular point'''\n",
    "    plt.figure(plotNo)\n",
    "    plt.plot(dataArray, sampleDepthAxis, label = label)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(str(title))\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "def tempGroupProfile(dataArrayMean, dataArrayStd, plotNo):\n",
    "    '''Displays mean /+-1 std temperature profiles for classes in dataArrayMean and dataArrayStd. Requires sampleDepthAxis'''\n",
    "    dataCompNo = len(dataArrayMean)   \n",
    "    columnNames = sampleDFSortMeans.columns.values\n",
    "    dataStart = np.where(columnNames == sampleDepthAxis[0].values)[0][0]\n",
    "    subPlotX = int(np.ceil(dataCompNo/5))\n",
    "    \n",
    "    plt.figure(plotNo, figsize=(35, 10*subPlotX))\n",
    "    plt.style.use(\"seaborn-darkgrid\")\n",
    "    palette = cm.coolwarm(np.linspace(0,1, dataCompNo))\n",
    "    \n",
    "    for i in range(dataCompNo):\n",
    "        meanT = dataArrayMean.iloc[i, dataStart:].to_numpy()\n",
    "        stdT = dataArrayStd.iloc[i, dataStart:].to_numpy()\n",
    "        \n",
    "        plt.subplot(subPlotX, 5, i+1)\n",
    "        plt.plot(meanT, sampleDepthAxis, marker='', linestyle=\"solid\", color=palette[i], linewidth=6.0, alpha=0.9)\n",
    "        plt.plot(meanT+stdT, sampleDepthAxis, marker='', linestyle=\"dashed\", color=palette[i], linewidth=6.0, alpha=0.9)\n",
    "        plt.plot(meanT-stdT, sampleDepthAxis, marker='', linestyle=\"dashed\", color=palette[i], linewidth=6.0, alpha=0.9)\n",
    "        \n",
    "        plt.xlim([-2,20])\n",
    "        plt.ylim([0,1000])\n",
    "        ax = plt.gca()\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        fs = 16 #font size\n",
    "        plt.xlabel(\"Temperature (Â°C)\", fontsize=fs)\n",
    "        plt.ylabel(\"Depth (m)\", fontsize=fs)\n",
    "        plt.title(\"Class = \"+str(i), fontsize=fs)\n",
    "        mpl.rc(\"xtick\", labelsize=fs)\n",
    "        mpl.rc(\"ytick\", labelsize=fs)\n",
    "        \n",
    "        '''\n",
    "        textstr = '\\n'.join((\n",
    "            r'N profs. = %i' % (nprofs[nrow], ),\n",
    "            r'Mean lon = %i' % (meanLon, ),\n",
    "            r'Mean lat = %i' % (meanLat, ),\n",
    "            r'Post. = %i' % (meanMaxPP, )))\n",
    "        props = dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8)\n",
    "        ax.text(0.45, 0.25, textstr, transform=ax.transAxes, fontsize=fs, verticalalignment='top', bbox=props)\n",
    "        '''\n",
    "\n",
    "\n",
    "print(\"Plotting functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a11ef",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70311e8",
   "metadata": {},
   "source": [
    "### Plotting Ocean Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8623ab",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Mask plotting cell\n",
    "#locationPlotXr(geoRangeFilt, (10,10), 1) #OceanMaskVolcello\n",
    "#locationPlotXr(geoRangeFilt2, (10,10), 2) #OceanMaskUKESM1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4026ed",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8a300",
   "metadata": {},
   "source": [
    "### Generating Data Samples\n",
    "<b>Identifying, masking and stacking raw data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703c272",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Identifying, masking and stacking raw data cell\n",
    "dfESMLatLevTStack = dfESMLatLevT.stack(ij =(\"i\", \"j\"))\n",
    "dfESMLatLevTStack = dfESMLatLevTStack.transpose('time', 'ij', 'lev')\n",
    "dfESMLatLevTStackFilt = dfESMLatLevTStack.sel(ij = geoRangeFilt.ij.values) #Produces 22194\n",
    "dfESMLatLevTStackFilt\n",
    "print(\"Raw data identified, stacked and stored in dfESMLatLevTStackFilt. Data dimensions: \"+str(dfESMLatLevTStackFilt.sizes)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135177d4",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Plotting raw data locations cell\n",
    "#locationPlotXr(dfESMLatLevTStackFilt, (10,10), 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d6150",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Selecting sample data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd28e7",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Mask loading cell\n",
    "if maskEnable:\n",
    "    importName = modelName + \"_Mask.npy\"\n",
    "    mask = np.load(importName)\n",
    "    print(\"Data mask loaded from \"+ importName +\".\")\n",
    "else:\n",
    "    print(\"No mask loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e67bc",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Selecting sample data cell\n",
    "sampleDataRaw = dfESMLatLevTStackFilt.reset_index('ij')\n",
    "sampleDataRaw = sampleDataRaw.stack(ijT = ('time', 'ij'))\n",
    "\n",
    "if maskEnable:\n",
    "    sampleData = sampleDataRaw[:,mask] #Training data mask applied\n",
    "else:\n",
    "    sampleData = sampleDataRaw #Full data set to be classified\n",
    "\n",
    "sampleData = sampleData.transpose('ijT', 'lev')\n",
    "print(\"Sample data calculated and stored in sampleData. Sample data dimensions: \"+str(sampleData.sizes)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0946ca0c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Placing sample data in tables</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cf15a",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Location and time data to table cell\n",
    "metaData = {\"lat\":sampleData[\"lat\"], \"lon\":sampleData[\"lon\"], \"time\":sampleData[\"time\"]}\n",
    "sampleMetaDF = pd.DataFrame(metaData, columns=[\"lat\", \"lon\", \"time\"])\n",
    "print(\"Sample lat, lon and time converted to datafile (sampleMetaDF). \"+str(len(sampleMetaDF))+\" samples identified.\")\n",
    "sampleMetaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28692e94",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Temperature data to table and table merging cell\n",
    "#Generating surface temperature level value and column name\n",
    "surfaceLev = sampleData[\"lev\"][0].values\n",
    "surfaceData = sampleData.sel(lev = surfaceLev)\n",
    "surfaceLevName = \"Surface Temp (\"+str(np.round(surfaceLev,2))+\")\"\n",
    "\n",
    "#Exporting sample data into pandas\n",
    "if True:\n",
    "    sampleDataDF = sampleData.to_pandas()\n",
    "    sampleDataDFClean = sampleDataDF.reset_index()\n",
    "    sampleDataDFClean = sampleDataDFClean.drop(columns=['ij'])\n",
    "    sampleDF = pd.concat([sampleMetaDF, sampleDataDFClean.drop(columns=[\"time\"])], axis=1) #Removes time from second table for merge\n",
    "else:\n",
    "    sampleDF = sampleMetaDF\n",
    "    \n",
    "sampleDF[\"time\"] = pd.to_datetime(sampleDF[\"time\"])\n",
    "print(\"SampleData converted to datafile (sampleDataDF).  Datafiles combined into sampleDF.\")\n",
    "sampleDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb22b3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4dd1f1",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "<b>Scaling implementation</b><br>\n",
    "Applying scaling to the data set, ensuring all levels have same influence over data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb41e4",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Scaler loading and transform cell\n",
    "importName = modelName + \"_Scaler\"\n",
    "scalerLoad = load(importName)\n",
    "sampleDataScaled = scalerLoad.transform(sampleData)\n",
    "print(\"Scaling of sampleData complete using \"+ importName +\", stored in sampleDataScaled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f1dfa",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Scaling comparison</b><br>\n",
    "Comparing raw temperature profiles with their scaled equivalent. To show individual plots set solo to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4767c01",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Scaled temperature profile plotting cell\n",
    "solo = False #Set to true for seperate plots, false for combined plots.\n",
    "for i in range(5):\n",
    "    x = np.random.randint(len(sampleMetaDF))\n",
    "    tempPointPlot(sampleData[x], x, \"sample raw \"+str(x), solo*2*i)\n",
    "    tempPointPlot(sampleDataScaled[x], x, \"sample scaled \"+str(x), solo*2*i+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df9715",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72188c33",
   "metadata": {},
   "source": [
    "### Principle Component Analysis\n",
    "This process is performed to reduce the number of dimensions of the the data, as well as to improve overall model\n",
    "performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab296ec5",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#PCA importing cell\n",
    "importName = modelName + \"_PCA.pkl\"\n",
    "pca = pk.load(open(importName, \"rb\"))\n",
    "totalVarianceExplained = np.sum(pca.explained_variance_ratio_)\n",
    "print(\"PCA loaded into pca. Total variance explained by PCA for \"+str(pca.n_components)+\" is \"+str(totalVarianceExplained)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789f8c2",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#PCA transform cell\n",
    "sampleDataScaledPCA = pca.transform(sampleDataScaled) #converting input data into PCA representation\n",
    "print(\"Data passed through PCA to sampleDataPCA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d98888",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5e7364",
   "metadata": {},
   "source": [
    "### Model import/BIC score calculation\n",
    "The previously generated model is imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d6011",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Model import and BIC + Component Calculation Cell\n",
    "bestGMModel = loadModel(modelName) #Loading model\n",
    "bicMin = bestGMModel.bic(sampleDataScaledPCA) #BIC score calculation\n",
    "bicComponentMin = bestGMModel.n_components #Identifying number of components in model (2 for this notebook)\n",
    "\n",
    "print(\"Model \"+modelName+\" loaded. The bicScore was \"+str(np.round(bicMin, 2))+\" for \"+str(bicComponentMin)+\".\")\n",
    "print(\"Imported model \"+modelName+\" in use. No calculations necessary.\")\n",
    "print(\"Imported model \"+modelName+\" in use. Model BIC score for training data: \"+str(bicMin)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2e6b1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db64938",
   "metadata": {},
   "source": [
    "### Assigning class labels to each profile using the best GMM\n",
    "Implementation of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d8266",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Classification and classification probability cell\n",
    "labels = bestGMModel.predict(sampleDataScaledPCA) #Assignment of class labels from best GMM\n",
    "posteriorProbs = bestGMModel.predict_proba(sampleDataScaledPCA) #Probability of profile belonging in class\n",
    "maxPosteriorProbs = np.max(posteriorProbs, axis=1) #Evaluating assigned class probability\n",
    "classUncertainty = 2 - 2*maxPosteriorProbs #I factor calculation for 2 class system (reduces second max lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51fc89",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Initial class labels to sampleDF table cell\n",
    "try: #Removing label, maxposteriorprob and classUncertainty columns from sampleDF\n",
    "    sampleDF = sampleDF.drop(columns=[\"label\", \"max posterior prob\", \"classUncertainty\"]) #removes any previous labels or probabilities\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Adding label, maxposteriorprob and classUncertainty columns to sampleDF\n",
    "sampleDF.insert(3, \"label\", labels, True)\n",
    "sampleDF.insert(4, \"max posterior prob\", maxPosteriorProbs, True)\n",
    "sampleDF.insert(5, \"classUncertainty\", classUncertainty, True)\n",
    "print(\"Labels identified for model (\"+str(bicComponentMin)+\" components) and added to sampleDF with associated probability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662c916",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e780c13",
   "metadata": {},
   "source": [
    "### Calculating class means for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98462b42",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Class Mean Calculation Cell\n",
    "sampleDFGrouped = sampleDF.groupby(\"label\") #group profiles according to label\n",
    "sampleDFMeans = sampleDFGrouped.mean() #calculate mean of all profiles in each class\n",
    "print(\"Sample dataframe grouped by label (sampleDFGrouped) and means taken (sampleDFMeans).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd4b34",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57a2c9",
   "metadata": {},
   "source": [
    "### Sorting the labels based on mean class temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e670c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Sorted Dictionary creation cell\n",
    "surfaceMeans = sampleDFMeans[surfaceLev].to_numpy() #Takes first temperature data column\n",
    "surfaceMeansOrder = np.argsort(surfaceMeans)\n",
    "di = dict(zip(surfaceMeansOrder, range(0, bicComponentMin)))\n",
    "print(\"Surface temperature means taken and sorted. Label dictionary created and stored in di.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80f04a",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Sorted label column to tables cell\n",
    "try: #Removing labelSorted column from tables\n",
    "    sampleMetaDF = sampleMetaDF.drop(columns = \"labelSorted\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    sampleDF = sampleDF.drop(columns = \"labelSorted\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Adding sorted label information to sampleMetaDF and sampleDF\n",
    "sampleMetaDF.insert(3, \"labelSorted\", sampleDF[\"label\"].map(di))\n",
    "sampleDF.insert(5, \"labelSorted\", sampleDF[\"label\"].map(di))\n",
    "print(\"Sorted labels assigned to sampleDF based on surface temperature, coldest to warmest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f7e2a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Probability data to sampleMeta table cell\n",
    "try:\n",
    "    sampleMetaDF = sampleMetaDF.drop(columns = [\"max posterior prob\", \"classUncertainty\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sampleMetaDF.insert(4, \"max posterior prob\", maxPosteriorProbs, True)\n",
    "sampleMetaDF.insert(5, \"classUncertainty\", classUncertainty, True)\n",
    "\n",
    "sampleMetaDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29dcc3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff2920",
   "metadata": {},
   "source": [
    "### Use pandas to calculate the properties of the profiles by sorted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c284e",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Class temperature means and stds cell\n",
    "sampleDFSortGrouped = sampleDF.groupby(\"labelSorted\")\n",
    "sampleDFSortMeans = sampleDFSortGrouped.mean()\n",
    "sampleDFSortStds = sampleDFSortGrouped.std()\n",
    "profileCount = sampleDFSortGrouped[sampleDF.columns[0]].count().to_numpy()\n",
    "print(\"sampleDF grouped by sorted label (sampleDFSortGrouped), with means and standard deviations calculated for each group (sampleDFSortMeans, sampleDFSortStd).\")\n",
    "print(\"Number of samples in each group calculated and stored in profileCount.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae37832",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6455a70",
   "metadata": {},
   "source": [
    "### Confirmation of sorting\n",
    "The means printed below should be ordered, going from coldest to warmest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16d8ae",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Temperature display cell\n",
    "print(sampleDFSortMeans[sampleDataDF.columns[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e10b1d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c70605",
   "metadata": {},
   "source": [
    "### Plotting the means and standard deviations of the classes by profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a64639",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Plotting mean and std profiles cell\n",
    "tempGroupProfile(sampleDFSortMeans, sampleDFSortStds, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b80fa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eeba57",
   "metadata": {},
   "source": [
    "### Plotting location and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07f32c",
   "metadata": {
    "code_folding": [],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#surfaceTempPlot(sampleDF, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb8537",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#locationPlotGroupDFLab(sampleDF, \"Location plot of grouping\", (25,25), 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cb451",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#locationPlotGroupDFMonthly(sampleDF, \"Monthly summaries for training data set\", 1)\n",
    "print(\"Classifications, grouped by month.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac924f77",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#locationPlotUncertaintyDFMonthly(sampleDF, \"Monthly uncertainty\", 1)\n",
    "print(\"Uncertainty in classifications, grouped by month.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390f14e",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6349509",
   "metadata": {},
   "source": [
    "### Exporting Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleMetaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e91870",
   "metadata": {
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Meta data export cell\n",
    "exportName = modelName + \"_Meta_Full\"\n",
    "sampleMetaDF.to_csv(exportName) #Exporting meta data\n",
    "print(\"Meta data and mask exported to \"+ exportName +\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1b78e",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Meta data reload cell\n",
    "importName = modelName + \"_Meta_Full\"\n",
    "sampleMetaReload = pd.read_csv(importName)\n",
    "print(\"Meta data reloaded from \"+ importName +\". \"+str(len(sampleMetaReload))+\" data points.\")\n",
    "sampleMetaReload.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605e0e1",
   "metadata": {},
   "source": [
    "### End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
